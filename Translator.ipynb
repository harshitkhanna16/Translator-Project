{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSVurpiRDJhbcd9thJ/CEs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitkhanna16/Translator-Project/blob/main/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y6K0GU9BMyxi"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "# --- Set Project Hyperparameters (FAST SETTINGS) ---\n",
        "\n",
        "# Size of the \"thought vector\" (Smaller = faster)\n",
        "LATENT_DIM = 128\n",
        "\n",
        "# Use fewer samples (Fewer = faster)\n",
        "NUM_SAMPLES = 5000\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10  # Reduced from 30 (Fewer = faster)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RUN THIS CELL TO FIX THE ERROR ---\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Attempting to clear the cached dataset...\")\n",
        "# The -f (force) flag prevents errors if the files don't exist\n",
        "!rm -f /root/.keras/datasets/spa-eng.zip\n",
        "!rm -rf /root/.keras/datasets/spa-eng\n",
        "\n",
        "print(\"✅ Cache cleared.\")\n",
        "print(\"Please re-run Cell 2 now.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcJrBMj0PSTj",
        "outputId": "9d6084d2-11c7-46a2-a797-fc807fe38177"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to clear the cached dataset...\n",
            "✅ Cache cleared.\n",
            "Please re-run Cell 2 now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "# --- Step 1: Define paths and clear cache ---\n",
        "dataset_dir = \"/root/.keras/datasets/\"\n",
        "zip_file_path = os.path.join(dataset_dir, \"spa-eng.zip\")\n",
        "extracted_dir_path = os.path.join(dataset_dir, \"spa-eng\")\n",
        "file_path = os.path.join(extracted_dir_path, \"spa.txt\") # This is the file we need\n",
        "\n",
        "print(\"Clearing any old, broken files...\")\n",
        "# The -f (force) flag prevents errors if the files don't exist\n",
        "!rm -f {zip_file_path}\n",
        "!rm -rf {extracted_dir_path}\n",
        "os.makedirs(dataset_dir, exist_ok=True) # Ensure the main directory exists\n",
        "print(\"Cache cleared.\")\n",
        "\n",
        "# --- Step 2: Manually Download the file ---\n",
        "url = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n",
        "print(f\"Downloading dataset from {url}...\")\n",
        "try:\n",
        "    urllib.request.urlretrieve(url, zip_file_path)\n",
        "    print(f\"Downloaded zip to: {zip_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during download: {e}\")\n",
        "\n",
        "# --- Step 3: Manually extract the file ---\n",
        "print(\"Extracting file...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_dir) # Extract to the /datasets/ directory\n",
        "    print(f\"Successfully extracted to: {extracted_dir_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# --- Step 4: Load the data (The rest of your original cell) ---\n",
        "# These lists will hold our sentences\n",
        "input_texts = []    # English sentences (Input)\n",
        "target_texts = []   # Spanish sentences (Target)\n",
        "\n",
        "# Read the file\n",
        "print(f\"Reading file from: {file_path}\")\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "\n",
        "    print(f\"Total lines in file: {len(lines)}\")\n",
        "\n",
        "    # Loop through the lines and process them\n",
        "    # We use the NUM_SAMPLES variable defined in Cell 1\n",
        "    for line in lines[:min(NUM_SAMPLES, len(lines) - 1)]:\n",
        "        try:\n",
        "            # Each line is \"English \\t Spanish \\t Attribution\"\n",
        "            input_text, target_text, _ = line.split('\\t')\n",
        "        except ValueError:\n",
        "            input_text, target_text = line.split('\\t')\n",
        "\n",
        "        # Add \"start\" and \"end\" tokens\n",
        "        target_text = '\\t' + target_text + '\\n'\n",
        "\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "    print(f\"Loaded {len(input_texts)} sentence pairs.\")\n",
        "    print(\"--- Example ---\")\n",
        "    print(\"Input (English):\", input_texts[0])\n",
        "    print(\"Target (Spanish):\", target_texts[0].strip())\n",
        "    print(\"\\n✅ Cell 2 successfully fixed and executed.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n--- ❌ ERROR ---\")\n",
        "    print(f\"File not found at: {file_path}\")\n",
        "    print(\"This means the extraction failed. Please check the output above for extraction errors.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stix8lLcPxw7",
        "outputId": "1fd948c1-e031-4c01-e1ea-07cfdc5f6429"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing any old, broken files...\n",
            "Cache cleared.\n",
            "Downloading dataset from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip...\n",
            "Downloaded zip to: /root/.keras/datasets/spa-eng.zip\n",
            "Extracting file...\n",
            "Successfully extracted to: /root/.keras/datasets/spa-eng\n",
            "Reading file from: /root/.keras/datasets/spa-eng/spa.txt\n",
            "Total lines in file: 118965\n",
            "Loaded 5000 sentence pairs.\n",
            "--- Example ---\n",
            "Input (English): Go.\n",
            "Target (Spanish): Ve.\n",
            "\n",
            "✅ Cell 2 successfully fixed and executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tokenize the Input (English) ---\n",
        "input_tokenizer = Tokenizer(filters='')\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "\n",
        "# --- Tokenize the Target (Spanish) ---\n",
        "# Fit the tokenizer BEFORE adding start/end tokens\n",
        "target_tokenizer = Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts([text.strip() for text in target_texts]) # Fit on stripped text\n",
        "\n",
        "# Manually add the start and end tokens to the tokenizer's vocabulary\n",
        "# Check if '\\t' and '\\n' are already in the vocabulary before adding\n",
        "if '\\t' not in target_tokenizer.word_index:\n",
        "    target_tokenizer.word_index['\\t'] = len(target_tokenizer.word_index) + 1\n",
        "if '\\n' not in target_tokenizer.word_index:\n",
        "    target_tokenizer.word_index['\\n'] = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Now convert texts to sequences AFTER adding tokens\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "\n",
        "# --- Get Vocabulary Sizes ---\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1 # Update size after adding tokens\n",
        "\n",
        "\n",
        "# --- Get Max Sequence Lengths ---\n",
        "max_encoder_seq_length = max(len(seq) for seq in input_sequences)\n",
        "max_decoder_seq_length = max(len(seq) for seq in target_sequences)\n",
        "\n",
        "print(f\"Input Vocab Size: {input_vocab_size}\")\n",
        "print(f\"Target Vocab Size: {target_vocab_size}\")\n",
        "print(f\"Max English sentence length: {max_encoder_seq_length}\")\n",
        "print(f\"Max Spanish sentence length: {max_decoder_seq_length}\")\n",
        "\n",
        "# --- Pad Sequences ---\n",
        "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
        "decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
        "\n",
        "print(\"--- Data Shapes ---\")\n",
        "print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
        "print(\"Decoder Input Shape:\", decoder_input_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D81P37qGP1Zc",
        "outputId": "0959c25c-3283-4d56-b5a4-e57234b2e42e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Vocab Size: 1904\n",
            "Target Vocab Size: 3778\n",
            "Max English sentence length: 4\n",
            "Max Spanish sentence length: 6\n",
            "--- Data Shapes ---\n",
            "Encoder Input Shape: (5000, 4)\n",
            "Decoder Input Shape: (5000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CRITICAL STEP: \"Teacher Forcing\" ---\n",
        "# Create the \"target\" data, which is the decoder input data shifted by one timestep.\n",
        "\n",
        "# 1. Create an array of zeros with the same shape\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "\n",
        "# 2. Shift the sequences\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
        "\n",
        "# 3. Add the final dimension\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "print(\"Decoder Target Shape (shifted):\", decoder_target_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_lk-6ZnP6xb",
        "outputId": "66e5662f-9110-4579-a1e7-d833c6ae17f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Target Shape (shifted): (5000, 6, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Encoder Layers ---\n",
        "encoder_embedding_layer = Embedding(input_vocab_size, LATENT_DIM, name='encoder_embedding')\n",
        "encoder_lstm_layer = LSTM(LATENT_DIM, return_state=True, name='encoder_lstm')\n",
        "\n",
        "# --- 2. Decoder Layers ---\n",
        "decoder_embedding_layer = Embedding(target_vocab_size, LATENT_DIM, name='decoder_embedding')\n",
        "decoder_lstm_layer = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_dense_layer = Dense(target_vocab_size, activation='softmax', name='decoder_dense')"
      ],
      "metadata": {
        "id": "trf-SicBP_Xj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define ENCODER ---\n",
        "encoder_inputs = Input(shape=(None,), name='encoder_input')\n",
        "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
        "_, state_h, state_c = encoder_lstm_layer(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# --- 2. Define DECODER ---\n",
        "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
        "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "decoder_outputs, _, _ = decoder_lstm_layer(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_outputs = decoder_dense_layer(decoder_outputs)\n",
        "\n",
        "# --- 3. Build the final model ---\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--- Model Built Successfully ---\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "fXiHEYqlQDpU",
        "outputId": "9e5882db-3f02-4999-e0f2-004db112e57b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Built Successfully ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m243,712\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m483,584\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │    \u001b[38;5;34m131,584\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m131,584\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m487,362\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m3778\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">243,712</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">483,584</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ encoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">487,362</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3778</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,477,826\u001b[0m (5.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,477,826</span> (5.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,477,826\u001b[0m (5.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,477,826</span> (5.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "\n",
        "# This is where the learning happens!\n",
        "# It's faster because EPOCHS=10, LATENT_DIM=128, and NUM_SAMPLES=5000\n",
        "history = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],  # Our two inputs\n",
        "    decoder_target_data,                       # Our one output\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dACuVDBVQIai",
        "outputId": "82392647-1ffb-43a4-c759-33ebb734cc81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs...\n",
            "Epoch 1/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9058 - loss: 4.3687 - val_accuracy: 0.9348 - val_loss: 0.6004\n",
            "Epoch 2/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9789 - loss: 0.2060 - val_accuracy: 0.9348 - val_loss: 0.5437\n",
            "Epoch 3/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9778 - loss: 0.1878 - val_accuracy: 0.9348 - val_loss: 0.5118\n",
            "Epoch 4/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9798 - loss: 0.1621 - val_accuracy: 0.9348 - val_loss: 0.4926\n",
            "Epoch 5/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9796 - loss: 0.1538 - val_accuracy: 0.9348 - val_loss: 0.4795\n",
            "Epoch 6/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9787 - loss: 0.1541 - val_accuracy: 0.9348 - val_loss: 0.4716\n",
            "Epoch 7/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9778 - loss: 0.1527 - val_accuracy: 0.9348 - val_loss: 0.4606\n",
            "Epoch 8/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9776 - loss: 0.1511 - val_accuracy: 0.9348 - val_loss: 0.4600\n",
            "Epoch 9/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9785 - loss: 0.1414 - val_accuracy: 0.9348 - val_loss: 0.4539\n",
            "Epoch 10/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9784 - loss: 0.1391 - val_accuracy: 0.9348 - val_loss: 0.4506\n",
            "--- Training Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. The Encoder Model ---\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# --- 2. The Decoder Model ---\n",
        "decoder_state_input_h = Input(shape=(LATENT_DIM,), name='decoder_state_h')\n",
        "decoder_state_input_c = Input(shape=(LATENT_DIM,), name='decoder_state_c')\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embedding_inf = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm_layer(\n",
        "    decoder_embedding_inf, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states_inf = [state_h_inf, state_c_inf]\n",
        "\n",
        "decoder_outputs_inf = decoder_dense_layer(decoder_outputs_inf)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,      # Inputs: word + old states\n",
        "    [decoder_outputs_inf] + decoder_states_inf     # Outputs: word_prediction + new states\n",
        ")\n",
        "\n",
        "print(\"--- Inference Models Built Successfully ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrVCgAILQOK6",
        "outputId": "36dbfa4e-9e80-4e2b-91c7-265c6498e822"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inference Models Built Successfully ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW, FIXED CELL 9 ---\n",
        "\n",
        "# Create reverse-lookup dictionaries to turn numbers back into words\n",
        "reverse_input_word_index = {v: k for k, v in input_tokenizer.word_index.items()}\n",
        "reverse_target_word_index = {v: k for k, v in target_tokenizer.word_index.items()}\n",
        "\n",
        "# Get the token IDs for our [START] and [END] tokens\n",
        "start_token_id = target_tokenizer.word_index['\\t']\n",
        "end_token_id = target_tokenizer.word_index['\\n']\n",
        "\n",
        "def translate_sentence(input_seq):\n",
        "    # 1. ENCODE the input sentence\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # 2. START the decoder\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token_id\n",
        "\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        # 3. PREDICT the next word\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # 4. GET the most likely word\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = reverse_target_word_index.get(sampled_token_index, '')\n",
        "\n",
        "        # 5. --- THIS IS THE FIX ---\n",
        "        # Stop if we predict [END], the PADDING token (index 0),\n",
        "        # or the sentence gets too long.\n",
        "        if (sampled_word == '\\n' or\n",
        "            sampled_token_index == 0 or\n",
        "            len(decoded_sentence.split()) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        # --- END OF FIX ---\n",
        "        else:\n",
        "             decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # 6. UPDATE for the next loop\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()"
      ],
      "metadata": {
        "id": "k_IVaDelQTnK"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}